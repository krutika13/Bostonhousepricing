{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krutika13/Bostonhousepricing/blob/main/LCM_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUVgily62Y_f",
        "outputId": "04e478f1-b9a2-4dd7-d09a-85c45fe5c653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 10, 256])\n",
            "Output shape: torch.Size([4, 10, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Base-LCM Architecture Components\n",
        "class PreNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps input embeddings to the model's hidden dimension after normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.scaler_mean = 0.0  # Placeholder for robust scaler mean\n",
        "        self.scaler_std = 1.0   # Placeholder for robust scaler std\n",
        "\n",
        "    def normalize(self, x):\n",
        "        return (x - self.scaler_mean) / self.scaler_std\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalize(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps hidden state outputs back to the embedding space with denormalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.scaler_mean = 0.0  # Placeholder for robust scaler mean\n",
        "        self.scaler_std = 1.0   # Placeholder for robust scaler std\n",
        "\n",
        "    def denormalize(self, x):\n",
        "        return x * self.scaler_std + self.scaler_mean\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.denormalize(x)\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Decoder-Only Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, dropout=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, hidden_dim))  # Positional encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pos_encoder[:, :seq_len]\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x)  # Self-attention in decoder layers\n",
        "        return x\n",
        "\n",
        "class BaseLCM(nn.Module):\n",
        "    \"\"\"\n",
        "    Base Large Concept Model (LCM):\n",
        "    - PreNet: Maps input embeddings to hidden space.\n",
        "    - TransformerDecoder: Autoregressively processes embeddings.\n",
        "    - PostNet: Maps output back to the embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim):\n",
        "        super(BaseLCM, self).__init__()\n",
        "        self.prenet = PreNet(input_dim, hidden_dim)\n",
        "        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim)\n",
        "        self.postnet = PostNet(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prenet(x)\n",
        "        x = self.transformer_decoder(x)\n",
        "        x = self.postnet(x)\n",
        "        return x\n",
        "\n",
        "# Testing the Base-LCM architecture\n",
        "def test_base_lcm():\n",
        "    batch_size = 4\n",
        "    sequence_length = 10\n",
        "    input_dim = 256  # SONAR embedding dimension (e.g., pre-encoded sentences)\n",
        "    hidden_dim = 512\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    ff_dim = 2048\n",
        "    output_dim = 256  # Output embedding dimension (same as input)\n",
        "\n",
        "    # Random input to simulate SONAR embeddings\n",
        "    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)\n",
        "\n",
        "    # Initialize and test Base-LCM\n",
        "    model = BaseLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim)\n",
        "    output_embeddings = model(input_embeddings)\n",
        "\n",
        "    print(\"Input shape:\", input_embeddings.shape)\n",
        "    print(\"Output shape:\", output_embeddings.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_base_lcm()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geoopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCxdFqov6XXW",
        "outputId": "68ef85cc-937a-4422-904c-e9112d1ccb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoopt\n",
            "  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from geoopt) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from geoopt) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->geoopt) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->geoopt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->geoopt) (3.0.2)\n",
            "Downloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geoopt\n",
            "Successfully installed geoopt-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n",
        "from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n",
        "\n",
        "# Base-LCM Architecture Components with Hyperbolic Space\n",
        "class PreNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps input embeddings to the model's hidden dimension in hyperbolic space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, manifold):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.manifold = manifold\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.manifold.expmap0(x)  # Map to hyperbolic space (Poincare Ball)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps hidden state outputs back to the embedding space from hyperbolic space.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, output_dim, manifold):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.logmap0(x)  # Map back to Euclidean space\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Decoder-Only Transformer operating in hyperbolic space.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads, num_layers, ff_dim, manifold, dropout=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model=hidden_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.manifold = manifold\n",
        "        self.pos_encoder = ManifoldParameter(torch.zeros(1, 512, hidden_dim), manifold=manifold)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.manifold.expmap0(x + self.pos_encoder[:,:seq_len])  # Ensure curvature is retained\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x)  # Self-attention in decoder layers\n",
        "        return x\n",
        "\n",
        "class HyperbolicLCM(nn.Module):\n",
        "    \"\"\"\n",
        "    Base Large Concept Model (LCM) with Hyperbolic Hidden Space.\n",
        "    - PreNet: Maps input embeddings to hyperbolic space.\n",
        "    - TransformerDecoder: Operates in hyperbolic space.\n",
        "    - PostNet: Maps back to Euclidean space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold):\n",
        "        super(HyperbolicLCM, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.prenet = PreNet(input_dim, hidden_dim, manifold)\n",
        "        self.transformer_decoder = TransformerDecoder(hidden_dim, num_heads, num_layers, ff_dim, manifold)\n",
        "        self.postnet = PostNet(hidden_dim, output_dim, manifold)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prenet(x)\n",
        "        x = self.transformer_decoder(x)\n",
        "        x = self.postnet(x)\n",
        "        return x\n",
        "\n",
        "# Cosine Similarity for Accuracy\n",
        "def compute_accuracy(predicted, target, threshold=0.5):\n",
        "    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n",
        "    correct = (cos_sim > threshold).float()\n",
        "    accuracy = correct.mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Adding noise to target embeddings\n",
        "def add_noise_to_embeddings(embeddings, noise_level=0.1):\n",
        "    noise = torch.randn_like(embeddings) * noise_level\n",
        "    return embeddings + noise\n",
        "\n",
        "# Testing the Hyperbolic-LCM Architecture\n",
        "def test_hyperbolic_lcm():\n",
        "    batch_size = 4\n",
        "    sequence_length = 10\n",
        "    input_dim = 256  # Input embedding dimension\n",
        "    hidden_dim = 512  # Hidden dimension in hyperbolic space\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    ff_dim = 2048\n",
        "    output_dim = 256  # Output embedding dimension\n",
        "    epochs = 5  # Number of epochs for training\n",
        "    noise_level = 0.05  # Noise level for targets\n",
        "\n",
        "    # Initialize the Poincare Ball Manifold\n",
        "    manifold = PoincareBall(c=1.0)  # Curvature = 1.0\n",
        "\n",
        "    # Random input to simulate embeddings\n",
        "    input_embeddings = torch.randn(batch_size, sequence_length, input_dim)\n",
        "\n",
        "    # Initialize the Hyperbolic-LCM Model\n",
        "    model = HyperbolicLCM(input_dim, hidden_dim, num_heads, num_layers, ff_dim, output_dim, manifold)\n",
        "\n",
        "    # Define the Riemannian Adam optimizer\n",
        "    optimizer = RiemannianAdam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Create Target Embeddings with Noise\n",
        "    target_embeddings = add_noise_to_embeddings(input_embeddings, noise_level=noise_level)\n",
        "\n",
        "    # Training Loop for Multiple Epochs\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_embeddings = model(input_embeddings)\n",
        "        loss = criterion(output_embeddings, target_embeddings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute Accuracy\n",
        "        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold=0.2)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_hyperbolic_lcm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8UDyVJx6RT0",
        "outputId": "9ee7caf3-88ed-463a-8b67-9975d14df3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 1.0234 | Accuracy: 0.00%\n",
            "Epoch 2/5 | Loss: 0.9577 | Accuracy: 50.00%\n",
            "Epoch 3/5 | Loss: 0.8828 | Accuracy: 100.00%\n",
            "Epoch 4/5 | Loss: 0.9442 | Accuracy: 60.00%\n",
            "Epoch 5/5 | Loss: 0.8602 | Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fILVgm0I8nl7",
        "outputId": "3ee6a0e3-295b-4322-a41f-8ddfdf752230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torchtext --yes\n",
        "!pip install torchtext --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDILDRIb83av",
        "outputId": "1280f3d5-3b58-4a3a-9d6e-6d75351a59a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n",
        "from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n",
        "\n",
        "# Base-LCM Architecture Components with Hyperbolic Space and Pyramid Structure\n",
        "class PyramidLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Represents one pyramid layer: compresses dimensionality in hyperbolic space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, manifold):\n",
        "        super(PyramidLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.expmap0(self.linear(x))  # Map to hyperbolic space with compression\n",
        "        return x\n",
        "\n",
        "class HyperbolicCube(nn.Module):\n",
        "    \"\"\"\n",
        "    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers_dims, manifold):\n",
        "        super(HyperbolicCube, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.pyramid_layers = nn.ModuleList([\n",
        "            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n",
        "            for i in range(len(layers_dims) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.pyramid_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class PreNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps input embeddings to the hidden dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, manifold):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.manifold.expmap0(x)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps output back to the embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, output_dim, manifold):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.logmap0(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class HyperbolicLCM(nn.Module):\n",
        "    \"\"\"\n",
        "    LCM with a Hyperbolic Cube as the hidden space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim, manifold):\n",
        "        super(HyperbolicLCM, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.prenet = PreNet(input_dim, hidden_dims[0], manifold)\n",
        "        self.hyperbolic_cube = HyperbolicCube(hidden_dims, manifold)\n",
        "        self.postnet = PostNet(hidden_dims[-1], output_dim, manifold)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prenet(x)\n",
        "        x = self.hyperbolic_cube(x)\n",
        "        x = self.postnet(x)\n",
        "        return x\n",
        "\n",
        "# Cosine Similarity for Accuracy\n",
        "def compute_accuracy(predicted, target, threshold=0.1):\n",
        "    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n",
        "    correct = (cos_sim > threshold).float()\n",
        "    accuracy = correct.mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Load GloVe Embeddings Manually\n",
        "def load_glove_embeddings(file_path, vocab_size=5000):\n",
        "    \"\"\"Load GloVe embeddings from a file for a small subset.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= vocab_size:\n",
        "                break\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Prepare Input Embeddings\n",
        "def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):\n",
        "    \"\"\"Randomly sample embeddings from the loaded GloVe vectors.\"\"\"\n",
        "    selected_vectors = torch.stack(\n",
        "        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]\n",
        "    )\n",
        "    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "    return input_embeddings\n",
        "\n",
        "# Testing Hyperbolic-LCM Architecture\n",
        "def test_hyperbolic_lcm():\n",
        "    batch_size = 4\n",
        "    sequence_length = 10\n",
        "    input_dim = 300  # GloVe embedding dimension\n",
        "    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions\n",
        "    output_dim = 300\n",
        "    epochs = 25\n",
        "    threshold = 0.1  # Cosine similarity threshold (lowered)\n",
        "    glove_file = \"glove.6B.300d.txt\"  # Path to GloVe embeddings file\n",
        "\n",
        "    # Initialize the Poincare Ball Manifold\n",
        "    manifold = PoincareBall(c=1.0)\n",
        "\n",
        "    # Load GloVe Embeddings\n",
        "    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)\n",
        "    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)\n",
        "\n",
        "    # Initialize Hyperbolic-LCM Model\n",
        "    model = HyperbolicLCM(input_dim, hidden_dims, num_heads=8, num_layers=6, ff_dim=2048, output_dim=output_dim, manifold=manifold)\n",
        "    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)  # Lowered learning rate\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Create slightly perturbed target embeddings\n",
        "    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01  # Reduced noise level\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_embeddings = model(input_embeddings)\n",
        "        loss = criterion(output_embeddings, target_embeddings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute Accuracy\n",
        "        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_hyperbolic_lcm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2HrYtgH8k0S",
        "outputId": "8fb11505-6bd8-4bc6-c001-4e013a34ec94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Loss: 0.1043 | Accuracy: 15.00%\n",
            "Epoch 2/25 | Loss: 0.1039 | Accuracy: 20.00%\n",
            "Epoch 3/25 | Loss: 0.1036 | Accuracy: 25.00%\n",
            "Epoch 4/25 | Loss: 0.1032 | Accuracy: 30.00%\n",
            "Epoch 5/25 | Loss: 0.1028 | Accuracy: 30.00%\n",
            "Epoch 6/25 | Loss: 0.1025 | Accuracy: 30.00%\n",
            "Epoch 7/25 | Loss: 0.1021 | Accuracy: 30.00%\n",
            "Epoch 8/25 | Loss: 0.1018 | Accuracy: 40.00%\n",
            "Epoch 9/25 | Loss: 0.1014 | Accuracy: 40.00%\n",
            "Epoch 10/25 | Loss: 0.1011 | Accuracy: 42.50%\n",
            "Epoch 11/25 | Loss: 0.1007 | Accuracy: 60.00%\n",
            "Epoch 12/25 | Loss: 0.1004 | Accuracy: 67.50%\n",
            "Epoch 13/25 | Loss: 0.1001 | Accuracy: 97.50%\n",
            "Epoch 14/25 | Loss: 0.0998 | Accuracy: 100.00%\n",
            "Epoch 15/25 | Loss: 0.0995 | Accuracy: 100.00%\n",
            "Epoch 16/25 | Loss: 0.0992 | Accuracy: 100.00%\n",
            "Epoch 17/25 | Loss: 0.0988 | Accuracy: 100.00%\n",
            "Epoch 18/25 | Loss: 0.0985 | Accuracy: 100.00%\n",
            "Epoch 19/25 | Loss: 0.0982 | Accuracy: 100.00%\n",
            "Epoch 20/25 | Loss: 0.0979 | Accuracy: 100.00%\n",
            "Epoch 21/25 | Loss: 0.0976 | Accuracy: 100.00%\n",
            "Epoch 22/25 | Loss: 0.0973 | Accuracy: 100.00%\n",
            "Epoch 23/25 | Loss: 0.0970 | Accuracy: 100.00%\n",
            "Epoch 24/25 | Loss: 0.0967 | Accuracy: 100.00%\n",
            "Epoch 25/25 | Loss: 0.0965 | Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n",
        "from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n",
        "\n",
        "# Base-LCM Architecture Components with Hyperbolic Space and Pyramid Structure\n",
        "class PyramidLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Represents one pyramid layer: compresses dimensionality in hyperbolic space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, manifold):\n",
        "        super(PyramidLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.expmap0(self.linear(x))  # Map to hyperbolic space with compression\n",
        "        return x\n",
        "\n",
        "class HyperbolicCube(nn.Module):\n",
        "    \"\"\"\n",
        "    Hyperbolic Cube: Multiple pyramid layers forming a cube-like structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers_dims, manifold):\n",
        "        super(HyperbolicCube, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.pyramid_layers = nn.ModuleList([\n",
        "            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n",
        "            for i in range(len(layers_dims) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.pyramid_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class PreNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps input embeddings to the hidden dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, manifold):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.manifold.expmap0(x)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps output back to the embedding space.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, output_dim, manifold):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.logmap0(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class HyperbolicLCM(nn.Module):\n",
        "    \"\"\"\n",
        "    LCM with a Hyperbolic Cube as the hidden space.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims, num_heads, num_layers, ff_dim, output_dim):\n",
        "        super(HyperbolicLCM, self).__init__()\n",
        "        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))  # Learnable curvature\n",
        "        self.manifold = PoincareBall(c=self.curvature)\n",
        "        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)\n",
        "        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)\n",
        "        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prenet(x)\n",
        "        x = self.hyperbolic_cube(x)\n",
        "        x = self.postnet(x)\n",
        "        return x\n",
        "\n",
        "# Cosine Similarity for Accuracy\n",
        "def compute_accuracy(predicted, target, threshold=0.1):\n",
        "    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n",
        "    correct = (cos_sim > threshold).float()\n",
        "    accuracy = correct.mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Load GloVe Embeddings Manually\n",
        "def load_glove_embeddings(file_path, vocab_size=5000):\n",
        "    \"\"\"Load GloVe embeddings from a file for a small subset.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= vocab_size:\n",
        "                break\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Prepare Input Embeddings\n",
        "def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):\n",
        "    \"\"\"Randomly sample embeddings from the loaded GloVe vectors.\"\"\"\n",
        "    selected_vectors = torch.stack(\n",
        "        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]\n",
        "    )\n",
        "    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "    return input_embeddings\n",
        "\n",
        "# Testing Hyperbolic-LCM Architecture\n",
        "def test_hyperbolic_lcm():\n",
        "    batch_size = 4\n",
        "    sequence_length = 10\n",
        "    input_dim = 300  # GloVe embedding dimension\n",
        "    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions\n",
        "    output_dim = 300\n",
        "    epochs = 40\n",
        "    threshold = 0.1  # Cosine similarity threshold (lowered)\n",
        "    glove_file = \"glove.6B.300d.txt\"  # Path to GloVe embeddings file\n",
        "\n",
        "    # Load GloVe Embeddings\n",
        "    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)\n",
        "    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)\n",
        "\n",
        "    # Initialize Hyperbolic-LCM Model\n",
        "    model = HyperbolicLCM(input_dim, hidden_dims, num_heads=8, num_layers=6, ff_dim=2048, output_dim=output_dim)\n",
        "    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)  # Lowered learning rate\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Create slightly perturbed target embeddings\n",
        "    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01  # Reduced noise level\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_embeddings = model(input_embeddings)\n",
        "        loss = criterion(output_embeddings, target_embeddings)\n",
        "        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01  # Regularization term for curvature\n",
        "        total_loss = loss + curvature_reg\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute Accuracy\n",
        "        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Curvature: {model.curvature.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_hyperbolic_lcm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0h6Cpqq_lro",
        "outputId": "f32aa53e-f78f-4d85-c5b5-c91f727959a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 | Loss: 0.1061 | Curvature: 0.5414 | Accuracy: 0.00%\n",
            "Epoch 2/40 | Loss: 0.1057 | Curvature: 0.5415 | Accuracy: 0.00%\n",
            "Epoch 3/40 | Loss: 0.1054 | Curvature: 0.5416 | Accuracy: 0.00%\n",
            "Epoch 4/40 | Loss: 0.1050 | Curvature: 0.5417 | Accuracy: 0.00%\n",
            "Epoch 5/40 | Loss: 0.1047 | Curvature: 0.5418 | Accuracy: 7.50%\n",
            "Epoch 6/40 | Loss: 0.1043 | Curvature: 0.5419 | Accuracy: 10.00%\n",
            "Epoch 7/40 | Loss: 0.1040 | Curvature: 0.5420 | Accuracy: 10.00%\n",
            "Epoch 8/40 | Loss: 0.1037 | Curvature: 0.5421 | Accuracy: 10.00%\n",
            "Epoch 9/40 | Loss: 0.1033 | Curvature: 0.5422 | Accuracy: 22.50%\n",
            "Epoch 10/40 | Loss: 0.1030 | Curvature: 0.5423 | Accuracy: 35.00%\n",
            "Epoch 11/40 | Loss: 0.1027 | Curvature: 0.5424 | Accuracy: 40.00%\n",
            "Epoch 12/40 | Loss: 0.1024 | Curvature: 0.5425 | Accuracy: 40.00%\n",
            "Epoch 13/40 | Loss: 0.1021 | Curvature: 0.5426 | Accuracy: 50.00%\n",
            "Epoch 14/40 | Loss: 0.1018 | Curvature: 0.5427 | Accuracy: 60.00%\n",
            "Epoch 15/40 | Loss: 0.1014 | Curvature: 0.5428 | Accuracy: 60.00%\n",
            "Epoch 16/40 | Loss: 0.1011 | Curvature: 0.5429 | Accuracy: 60.00%\n",
            "Epoch 17/40 | Loss: 0.1009 | Curvature: 0.5430 | Accuracy: 70.00%\n",
            "Epoch 18/40 | Loss: 0.1006 | Curvature: 0.5431 | Accuracy: 70.00%\n",
            "Epoch 19/40 | Loss: 0.1003 | Curvature: 0.5432 | Accuracy: 70.00%\n",
            "Epoch 20/40 | Loss: 0.1000 | Curvature: 0.5433 | Accuracy: 70.00%\n",
            "Epoch 21/40 | Loss: 0.0997 | Curvature: 0.5434 | Accuracy: 80.00%\n",
            "Epoch 22/40 | Loss: 0.0994 | Curvature: 0.5435 | Accuracy: 87.50%\n",
            "Epoch 23/40 | Loss: 0.0991 | Curvature: 0.5436 | Accuracy: 90.00%\n",
            "Epoch 24/40 | Loss: 0.0988 | Curvature: 0.5437 | Accuracy: 90.00%\n",
            "Epoch 25/40 | Loss: 0.0985 | Curvature: 0.5438 | Accuracy: 92.50%\n",
            "Epoch 26/40 | Loss: 0.0982 | Curvature: 0.5439 | Accuracy: 100.00%\n",
            "Epoch 27/40 | Loss: 0.0979 | Curvature: 0.5440 | Accuracy: 100.00%\n",
            "Epoch 28/40 | Loss: 0.0977 | Curvature: 0.5441 | Accuracy: 100.00%\n",
            "Epoch 29/40 | Loss: 0.0974 | Curvature: 0.5442 | Accuracy: 100.00%\n",
            "Epoch 30/40 | Loss: 0.0971 | Curvature: 0.5443 | Accuracy: 100.00%\n",
            "Epoch 31/40 | Loss: 0.0968 | Curvature: 0.5444 | Accuracy: 100.00%\n",
            "Epoch 32/40 | Loss: 0.0965 | Curvature: 0.5445 | Accuracy: 100.00%\n",
            "Epoch 33/40 | Loss: 0.0962 | Curvature: 0.5446 | Accuracy: 100.00%\n",
            "Epoch 34/40 | Loss: 0.0959 | Curvature: 0.5447 | Accuracy: 100.00%\n",
            "Epoch 35/40 | Loss: 0.0956 | Curvature: 0.5448 | Accuracy: 100.00%\n",
            "Epoch 36/40 | Loss: 0.0953 | Curvature: 0.5449 | Accuracy: 100.00%\n",
            "Epoch 37/40 | Loss: 0.0950 | Curvature: 0.5450 | Accuracy: 100.00%\n",
            "Epoch 38/40 | Loss: 0.0947 | Curvature: 0.5451 | Accuracy: 100.00%\n",
            "Epoch 39/40 | Loss: 0.0944 | Curvature: 0.5452 | Accuracy: 100.00%\n",
            "Epoch 40/40 | Loss: 0.0941 | Curvature: 0.5453 | Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from geoopt import PoincareBall, ManifoldParameter  # For hyperbolic embeddings\n",
        "from geoopt.optim import RiemannianAdam  # Hyperbolic optimizer\n",
        "\n",
        "# Cosine Similarity for Accuracy\n",
        "def compute_accuracy(predicted, target, threshold=0.1):\n",
        "    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n",
        "    correct = (cos_sim > threshold).float()\n",
        "    accuracy = correct.mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Load GloVe Embeddings Manually\n",
        "def load_glove_embeddings(file_path, vocab_size=5000):\n",
        "    \"\"\"Load GloVe embeddings from a file for a small subset.\"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= vocab_size:\n",
        "                break\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Prepare Input Embeddings\n",
        "def prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=300):\n",
        "    \"\"\"Randomly sample embeddings from the loaded GloVe vectors.\"\"\"\n",
        "    selected_vectors = torch.stack(\n",
        "        [glove_embeddings[word] for word in list(glove_embeddings.keys())[:sequence_length]]\n",
        "    )\n",
        "    input_embeddings = selected_vectors.unsqueeze(0).repeat(batch_size, 1, 1)\n",
        "    return input_embeddings\n",
        "\n",
        "# Pyramid Layer\n",
        "class PyramidLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, manifold):\n",
        "        super(PyramidLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.expmap0(self.linear(x))  # Hyperbolic compression\n",
        "        return x\n",
        "\n",
        "# Hyperbolic Cube\n",
        "class HyperbolicCube(nn.Module):\n",
        "    def __init__(self, layers_dims, manifold):\n",
        "        super(HyperbolicCube, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.pyramid_layers = nn.ModuleList([\n",
        "            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n",
        "            for i in range(len(layers_dims) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.pyramid_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# PreNet and PostNet\n",
        "class PreNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, manifold):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.manifold.expmap0(x)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, manifold):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.logmap0(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Dual Hidden LCM with two hidden dimensions\n",
        "class DualHiddenLCM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, hidden_dim2, output_dim):\n",
        "        super(DualHiddenLCM, self).__init__()\n",
        "        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
        "        self.manifold = PoincareBall(c=self.curvature)\n",
        "\n",
        "        # Hidden Dimension 1: Pyramid structure\n",
        "        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)\n",
        "        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)\n",
        "        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)\n",
        "\n",
        "        # Hidden Dimension 2: 20D bottleneck\n",
        "        self.hidden_dim2 = nn.Linear(input_dim, hidden_dim2)\n",
        "        self.hidden_dim2_output = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hidden Dimension 1\n",
        "        x_hidden1 = self.prenet(x)\n",
        "        x_hidden1 = self.hyperbolic_cube(x_hidden1)\n",
        "        x_hidden1 = self.postnet(x_hidden1)\n",
        "\n",
        "        # Hidden Dimension 2\n",
        "        x_hidden2 = F.relu(self.hidden_dim2(x))\n",
        "        x_hidden2 = self.hidden_dim2_output(x_hidden2)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = x_hidden1 + x_hidden2\n",
        "        return combined\n",
        "\n",
        "# Testing DualHiddenLCM Architecture\n",
        "def test_dualhidden_lcm():\n",
        "    batch_size = 4\n",
        "    sequence_length = 10\n",
        "    input_dim = 300  # GloVe embedding dimension\n",
        "    hidden_dims = [512, 256, 128, 64]  # Pyramid structure dimensions\n",
        "    hidden_dim2 = 20  # 20D bottleneck\n",
        "    output_dim = 300\n",
        "    epochs = 60\n",
        "    threshold = 0.1  # Cosine similarity threshold\n",
        "    glove_file = \"glove.6B.300d.txt\"  # Path to GloVe embeddings file\n",
        "\n",
        "    # Load GloVe Embeddings\n",
        "    glove_embeddings = load_glove_embeddings(glove_file, vocab_size=100)\n",
        "    input_embeddings = prepare_embeddings(glove_embeddings, batch_size, sequence_length, dim=input_dim)\n",
        "\n",
        "    # Initialize DualHiddenLCM Model\n",
        "    model = DualHiddenLCM(input_dim, hidden_dims, hidden_dim2, output_dim)\n",
        "    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Create slightly perturbed target embeddings\n",
        "    target_embeddings = input_embeddings + torch.randn_like(input_embeddings) * 0.01\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output_embeddings = model(input_embeddings)\n",
        "        loss = criterion(output_embeddings, target_embeddings)\n",
        "        curvature_reg = torch.abs(model.curvature - 1.0) * 0.01  # Regularization for curvature\n",
        "        total_loss = loss + curvature_reg\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute Accuracy\n",
        "        accuracy = compute_accuracy(output_embeddings, target_embeddings, threshold)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f} | Curvature: {model.curvature.item():.4f} | Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_dualhidden_lcm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyaSiwDo1nmo",
        "outputId": "31279c36-7e3c-4439-977d-a11547fb72bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60 | Loss: 0.1202 | Curvature: 0.5414 | Accuracy: 40.00%\n",
            "Epoch 2/60 | Loss: 0.1195 | Curvature: 0.5415 | Accuracy: 40.00%\n",
            "Epoch 3/60 | Loss: 0.1188 | Curvature: 0.5416 | Accuracy: 40.00%\n",
            "Epoch 4/60 | Loss: 0.1181 | Curvature: 0.5417 | Accuracy: 45.00%\n",
            "Epoch 5/60 | Loss: 0.1174 | Curvature: 0.5418 | Accuracy: 45.00%\n",
            "Epoch 6/60 | Loss: 0.1168 | Curvature: 0.5419 | Accuracy: 50.00%\n",
            "Epoch 7/60 | Loss: 0.1161 | Curvature: 0.5420 | Accuracy: 50.00%\n",
            "Epoch 8/60 | Loss: 0.1155 | Curvature: 0.5421 | Accuracy: 55.00%\n",
            "Epoch 9/60 | Loss: 0.1149 | Curvature: 0.5422 | Accuracy: 60.00%\n",
            "Epoch 10/60 | Loss: 0.1143 | Curvature: 0.5423 | Accuracy: 60.00%\n",
            "Epoch 11/60 | Loss: 0.1137 | Curvature: 0.5424 | Accuracy: 60.00%\n",
            "Epoch 12/60 | Loss: 0.1132 | Curvature: 0.5425 | Accuracy: 60.00%\n",
            "Epoch 13/60 | Loss: 0.1126 | Curvature: 0.5426 | Accuracy: 60.00%\n",
            "Epoch 14/60 | Loss: 0.1121 | Curvature: 0.5427 | Accuracy: 60.00%\n",
            "Epoch 15/60 | Loss: 0.1116 | Curvature: 0.5428 | Accuracy: 60.00%\n",
            "Epoch 16/60 | Loss: 0.1111 | Curvature: 0.5429 | Accuracy: 60.00%\n",
            "Epoch 17/60 | Loss: 0.1106 | Curvature: 0.5430 | Accuracy: 60.00%\n",
            "Epoch 18/60 | Loss: 0.1101 | Curvature: 0.5431 | Accuracy: 60.00%\n",
            "Epoch 19/60 | Loss: 0.1096 | Curvature: 0.5432 | Accuracy: 70.00%\n",
            "Epoch 20/60 | Loss: 0.1091 | Curvature: 0.5433 | Accuracy: 70.00%\n",
            "Epoch 21/60 | Loss: 0.1086 | Curvature: 0.5434 | Accuracy: 70.00%\n",
            "Epoch 22/60 | Loss: 0.1082 | Curvature: 0.5435 | Accuracy: 70.00%\n",
            "Epoch 23/60 | Loss: 0.1077 | Curvature: 0.5436 | Accuracy: 70.00%\n",
            "Epoch 24/60 | Loss: 0.1073 | Curvature: 0.5437 | Accuracy: 77.50%\n",
            "Epoch 25/60 | Loss: 0.1068 | Curvature: 0.5438 | Accuracy: 80.00%\n",
            "Epoch 26/60 | Loss: 0.1064 | Curvature: 0.5439 | Accuracy: 80.00%\n",
            "Epoch 27/60 | Loss: 0.1059 | Curvature: 0.5440 | Accuracy: 80.00%\n",
            "Epoch 28/60 | Loss: 0.1055 | Curvature: 0.5441 | Accuracy: 80.00%\n",
            "Epoch 29/60 | Loss: 0.1050 | Curvature: 0.5442 | Accuracy: 97.50%\n",
            "Epoch 30/60 | Loss: 0.1046 | Curvature: 0.5443 | Accuracy: 100.00%\n",
            "Epoch 31/60 | Loss: 0.1042 | Curvature: 0.5444 | Accuracy: 100.00%\n",
            "Epoch 32/60 | Loss: 0.1037 | Curvature: 0.5445 | Accuracy: 100.00%\n",
            "Epoch 33/60 | Loss: 0.1033 | Curvature: 0.5446 | Accuracy: 100.00%\n",
            "Epoch 34/60 | Loss: 0.1029 | Curvature: 0.5447 | Accuracy: 100.00%\n",
            "Epoch 35/60 | Loss: 0.1024 | Curvature: 0.5448 | Accuracy: 100.00%\n",
            "Epoch 36/60 | Loss: 0.1020 | Curvature: 0.5449 | Accuracy: 100.00%\n",
            "Epoch 37/60 | Loss: 0.1016 | Curvature: 0.5450 | Accuracy: 100.00%\n",
            "Epoch 38/60 | Loss: 0.1012 | Curvature: 0.5451 | Accuracy: 100.00%\n",
            "Epoch 39/60 | Loss: 0.1007 | Curvature: 0.5452 | Accuracy: 100.00%\n",
            "Epoch 40/60 | Loss: 0.1003 | Curvature: 0.5453 | Accuracy: 100.00%\n",
            "Epoch 41/60 | Loss: 0.0999 | Curvature: 0.5454 | Accuracy: 100.00%\n",
            "Epoch 42/60 | Loss: 0.0995 | Curvature: 0.5455 | Accuracy: 100.00%\n",
            "Epoch 43/60 | Loss: 0.0990 | Curvature: 0.5456 | Accuracy: 100.00%\n",
            "Epoch 44/60 | Loss: 0.0986 | Curvature: 0.5457 | Accuracy: 100.00%\n",
            "Epoch 45/60 | Loss: 0.0982 | Curvature: 0.5458 | Accuracy: 100.00%\n",
            "Epoch 46/60 | Loss: 0.0978 | Curvature: 0.5459 | Accuracy: 100.00%\n",
            "Epoch 47/60 | Loss: 0.0973 | Curvature: 0.5460 | Accuracy: 100.00%\n",
            "Epoch 48/60 | Loss: 0.0969 | Curvature: 0.5461 | Accuracy: 100.00%\n",
            "Epoch 49/60 | Loss: 0.0965 | Curvature: 0.5462 | Accuracy: 100.00%\n",
            "Epoch 50/60 | Loss: 0.0961 | Curvature: 0.5463 | Accuracy: 100.00%\n",
            "Epoch 51/60 | Loss: 0.0956 | Curvature: 0.5464 | Accuracy: 100.00%\n",
            "Epoch 52/60 | Loss: 0.0952 | Curvature: 0.5465 | Accuracy: 100.00%\n",
            "Epoch 53/60 | Loss: 0.0948 | Curvature: 0.5466 | Accuracy: 100.00%\n",
            "Epoch 54/60 | Loss: 0.0944 | Curvature: 0.5467 | Accuracy: 100.00%\n",
            "Epoch 55/60 | Loss: 0.0939 | Curvature: 0.5468 | Accuracy: 100.00%\n",
            "Epoch 56/60 | Loss: 0.0935 | Curvature: 0.5469 | Accuracy: 100.00%\n",
            "Epoch 57/60 | Loss: 0.0931 | Curvature: 0.5470 | Accuracy: 100.00%\n",
            "Epoch 58/60 | Loss: 0.0926 | Curvature: 0.5471 | Accuracy: 100.00%\n",
            "Epoch 59/60 | Loss: 0.0922 | Curvature: 0.5472 | Accuracy: 100.00%\n",
            "Epoch 60/60 | Loss: 0.0918 | Curvature: 0.5473 | Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from geoopt import PoincareBall, ManifoldParameter\n",
        "from geoopt.optim import RiemannianAdam\n",
        "\n",
        "# Initialize DDP\n",
        "def setup_ddp(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "    torch.cuda.set_device(rank)\n",
        "\n",
        "# Cleanup DDP\n",
        "def cleanup_ddp():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "# Cosine Similarity for Accuracy\n",
        "def compute_accuracy(predicted, target, threshold=0.1):\n",
        "    cos_sim = F.cosine_similarity(predicted, target, dim=-1)\n",
        "    correct = (cos_sim > threshold).float()\n",
        "    accuracy = correct.mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# DualHiddenLCM Definition\n",
        "class DualHiddenLCM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, hidden_dim2, output_dim):\n",
        "        super(DualHiddenLCM, self).__init__()\n",
        "        self.curvature = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
        "        self.manifold = PoincareBall(c=self.curvature)\n",
        "\n",
        "        # Hidden Dimension 1: Pyramid structure\n",
        "        self.prenet = PreNet(input_dim, hidden_dims[0], self.manifold)\n",
        "        self.hyperbolic_cube = HyperbolicCube(hidden_dims, self.manifold)\n",
        "        self.postnet = PostNet(hidden_dims[-1], output_dim, self.manifold)\n",
        "\n",
        "        # Hidden Dimension 2: 20D bottleneck\n",
        "        self.hidden_dim2 = nn.Linear(input_dim, hidden_dim2)\n",
        "        self.hidden_dim2_output = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Hidden Dimension 1\n",
        "        x_hidden1 = self.prenet(x)\n",
        "        x_hidden1 = self.hyperbolic_cube(x_hidden1)\n",
        "        x_hidden1 = self.postnet(x_hidden1)\n",
        "\n",
        "        # Hidden Dimension 2\n",
        "        x_hidden2 = F.relu(self.hidden_dim2(x))\n",
        "        x_hidden2 = self.hidden_dim2_output(x_hidden2)\n",
        "\n",
        "        # Combine outputs\n",
        "        combined = x_hidden1 + x_hidden2\n",
        "        return combined\n",
        "\n",
        "# Helper Classes\n",
        "class PreNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, manifold):\n",
        "        super(PreNet, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.manifold.expmap0(x)\n",
        "        return x\n",
        "\n",
        "class PostNet(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, manifold):\n",
        "        super(PostNet, self).__init__()\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.logmap0(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class HyperbolicCube(nn.Module):\n",
        "    def __init__(self, layers_dims, manifold):\n",
        "        super(HyperbolicCube, self).__init__()\n",
        "        self.manifold = manifold\n",
        "        self.pyramid_layers = nn.ModuleList([\n",
        "            PyramidLayer(layers_dims[i], layers_dims[i+1], manifold)\n",
        "            for i in range(len(layers_dims) - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.pyramid_layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class PyramidLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, manifold):\n",
        "        super(PyramidLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.manifold = manifold\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.manifold.expmap0(self.linear(x))\n",
        "        return x\n",
        "\n",
        "# Training Loop\n",
        "def train(rank, world_size, data, target, input_dim, hidden_dims, hidden_dim2, output_dim, epochs, threshold):\n",
        "    setup_ddp(rank, world_size)\n",
        "    device = torch.device(f'cuda:{rank}')\n",
        "\n",
        "    # Prepare Model\n",
        "    model = DualHiddenLCM(input_dim, hidden_dims, hidden_dim2, output_dim).to(device)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    # Optimizer and Loss\n",
        "    optimizer = RiemannianAdam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # DataLoader\n",
        "    dataset = TensorDataset(data, target)\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_data, batch_target in dataloader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            batch_target = batch_target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_data)\n",
        "            loss = criterion(output, batch_target)\n",
        "            curvature_reg = torch.abs(model.module.curvature - 1.0) * 0.01  # Regularization\n",
        "            total_loss = loss + curvature_reg\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if rank == 0:  # Print only on the main process\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    cleanup_ddp()\n",
        "\n",
        "# Main Script\n",
        "def main():\n",
        "    world_size = torch.cuda.device_count()\n",
        "    if world_size < 2:\n",
        "        print(\"This script requires at least 2 GPUs.\")\n",
        "        return\n",
        "\n",
        "    # Simulated Data\n",
        "    input_dim = 300\n",
        "    hidden_dims = [512, 256, 128, 64]\n",
        "    hidden_dim2 = 20\n",
        "    output_dim = 300\n",
        "    epochs = 10\n",
        "    threshold = 0.1\n",
        "    batch_size = 4\n",
        "    num_samples = 100\n",
        "\n",
        "    data = torch.randn(num_samples, input_dim)\n",
        "    target = data + torch.randn_like(data) * 0.01  # Slight perturbation\n",
        "\n",
        "    # Start Training\n",
        "    torch.multiprocessing.spawn(\n",
        "        train,\n",
        "        args=(world_size, data, target, input_dim, hidden_dims, hidden_dim2, output_dim, epochs, threshold),\n",
        "        nprocs=world_size,\n",
        "        join=True\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "w4SIrFLXSpiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}